# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOxylO5LqcHrywdYDrFBHh8yE_7tHwi4
"""

pip install pandas spacy transformers torch scikit-learn

pip install en_core_web_sm

import pandas as pd
import spacy
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json

# Load JSON data
def load_json(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    return pd.DataFrame(data)

# Load business, checkin, tip, review, and user data
business_df = load_json('/content/yelp_academic_dataset_business.json')
checkin_df = load_json('/content/yelp_academic_dataset_checkin.json')
tip_df = load_json('/content/yelp_academic_dataset_tip.json')
review_df = load_json('review.json')
user_df = load_json('user.json')

# Check the first few rows of the review data
print(review_df.head())

# Keep relevant columns from the review dataset
review_df = review_df[['review_id', 'text', 'stars', 'business_id']]

# Load spaCy model for NLP tasks
import spacy
nlp = spacy.load("en_core_web_sm")

# Predefined business aspects
aspects = ['food quality', 'service', 'ambiance', 'pricing', 'cleanliness']

# Cleaning the review text (optional but recommended)
def clean_text(text):
    doc = nlp(text.lower())
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    return " ".join(tokens)

# Apply cleaning to the review text
review_df['cleaned_text'] = review_df['text'].apply(clean_text)

# Extracting predefined aspects from reviews
def extract_aspects(review):
    doc = nlp(review)
    extracted_aspects = []
    for token in doc:
        # Check if the token matches any predefined aspects
        if token.lemma_ in aspects:
            extracted_aspects.append(token.lemma_)
    return extracted_aspects

# Apply aspect extraction to the reviews
review_df['aspects'] = review_df['cleaned_text'].apply(extract_aspects)

from transformers import pipeline

# Load the pre-trained sentiment analysis model
sentiment_pipeline = pipeline("sentiment-analysis")

# Sentiment classification function
def classify_sentiment(aspect, review):
    # Consider the part of the review mentioning the aspect
    aspect_review = f"{aspect}: {review}"
    sentiment = sentiment_pipeline(aspect_review)[0]
    return sentiment['label']

# Classify sentiment for each extracted aspect in the review
review_df['aspect_sentiments'] = review_df.apply(
    lambda x: [(aspect, classify_sentiment(aspect, x['cleaned_text'])) for aspect in x['aspects']], axis=1
)

def generate_insights(df):
    insights = {}
    for aspect in aspects:
        # Filter reviews mentioning the specific aspect
        aspect_reviews = df[df['aspects'].apply(lambda x: aspect in x)]

        # Count positive, neutral, and negative sentiments for the aspect
        sentiment_counts = aspect_reviews['aspect_sentiments'].apply(lambda x: [s[1] for s in x if s[0] == aspect])
        sentiment_counts = sentiment_counts.explode().value_counts()

        # Save the counts to insights dictionary
        insights[aspect] = {
            'positive': sentiment_counts.get('POSITIVE', 0),
            'neutral': sentiment_counts.get('NEUTRAL', 0),
            'negative': sentiment_counts.get('NEGATIVE', 0)
        }

    return insights

# Generate insights for businesses
business_insights = generate_insights(review_df)

# Display the insights
for aspect, counts in business_insights.items():
    print(f"\nAspect: {aspect.capitalize()}")
    print(f"Positive reviews: {counts['positive']}")
    print(f"Neutral reviews: {counts['neutral']}")
    print(f"Negative reviews: {counts['negative']}")

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score

# Suppose we have ground truth sentiment labels for evaluation
y_true = review_df['true_sentiments']  # Add the true labels if available
y_pred = review_df['aspect_sentiments'].apply(lambda x: [sent for _, sent in x])

# Flatten the lists of true and predicted labels
y_true = [item for sublist in y_true for item in sublist]
y_pred = [item for sublist in y_pred for item in sublist]

# Calculate evaluation metrics
precision = precision_score(y_true, y_pred, average='macro')
recall = recall_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

print(f"Precision: {precision}, Recall: {recall}, F1-Score: {f1}")

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.show()

# Accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy}")

import matplotlib.pyplot as plt
import seaborn as sns

# Convert insights into a DataFrame for plotting
insights_df = pd.DataFrame(business_insights).T

# Plot aspect sentiment distribution
insights_df.plot(kind='bar', stacked=True)
plt.title('Aspect Sentiment Distribution')
plt.xlabel('Aspects')
plt.ylabel('Count of Sentiments')
plt.legend(title='Sentiment')
plt.show()